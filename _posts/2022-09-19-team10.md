---
layout: post
comments: true
title: Human-AI Shared Control via Policy Dissection
author: Ruikang Wu and Mengyuan Zhang (Team 10)
date: 2022-10-19
---

> In many complex tasks, RL-trained policy may not solve the tasks efficiently and correctly. The training process may cost too much time. Policy dissection is a frequency-based method, which can convert RL-trained policy into target-conditioned policy. For this method, human can interacte with AI inorder to get a more good and efficeient result. In this project, we want to explore the human-AI control and implementation of the policy dissection. We will add new action in MetaDrive enviornment.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
This article will refer to the [source code](https://github.com/metadriverse/policydissect) .

First of all, we want to explore the relationship between kinematic behavior and the neural activities. In this experiment, we want to figure out the patterns behind some certain behaviors. 

Second, we want to train bipedal robots, Cassie, in [IsaacGym](https://github.com/NVIDIA-Omniverse/IsaacGymEnvs), based on the patterns that we figured out. The robots will be trained to have some complex behaviors such as crouch, turn left, turn right, back flip, tiptoe, jump etc. 

Third, we want to explore whether the bipedal robots can handle more complex situtions within the human instruction.

Fourth, we want to explore whether we can improve the algoirthm. For improving the algorithm, we may potentially compare the policy based, value-based and model based methods, and choose the best performance methods.

Finally, we will try to figure out whther we can implement the policy dissection to the other complex tasks in the real world. We are still working on finding the real world problems. 

## Human-AI Shared Control
The present human-AI shared chontrol methods can be roughly divided into two categories. The first category is involving training with a human control and testing the trained policy withou the human control [5]. The other is to have human carry out human auxiliary tasks throughout both training and testing. In policy dissestion method, it allows the human cooperate with AI during the testing, but human does not participate in training.

In the test, humans need to trigger some subtle operations when necessary, such as shifting to the left and jumping, to help better complete the task.

## Policy Dissection
- What is the Policy Dissection?

    Previous reinforcement learning methods have attempted to design goals-conditioned policies that can achieve human goals, but at the cost of redesigning reward functions and training paradigms. Hence the Policy Dissection, an approach inspired by neuroscience methods based on the primate motor cortex. Using this approach, manually controllable policies can be derived in trained reinforcement learning agents. Experiments show that under the blessing of Policy Dissection, in MetaDrive, it can improve performance and security.

- Four main steps for Policy Dissection[1]

    **1. Monitoring Neural Activity and Kinematics:** The policy dissection expands the trained policy and records the tracked neural activities and kinematic attributes, such as velocity and yaw. It will recorde the neural activities and kinematic attributes for further anaylsis.

    **2. Associating Units with Kinematic Attributes:** According to the records of neural activities and kinematic attributes, the same kinematic patterns will appear with different frequencies. So for a kinematic patterns, the unit with the smallest frequency discrepancy is the motor primitive. 

    **3. Building Stimulation-evoked Map:** Behavior can be described by changing a subset of kinematic attributes, or by activating a corresponding set of motor primitives. These movements are associated with certain behaviors to generate building blocks that include stimulation-evoked map.

    **4. Steering Agent via Stimulation-evoked Map:** Activate the kinematic attribute associated with the motion property and apply its derivative to the agent. The goal is to find features that can be easily scaled up or down kinematic attributes by selecting a unit.

- Workflow of Policy Dissection

    ![workflow]({{ '/assets/images/team10/algorithm.png'| relative_url}})
    {: style="width: 400px; max-width: 100%;"}
    *Fig 1. workflow of Policy Dissection* [1].

## PPO Result on MetaDrive
![ppo demo1]({{ '/assets/images/team10/demo1.gif'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 2. Demo 1*

![ppo demo2]({{ '/assets/images/team10/demo2.gif'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 3. Demo 2*

![ppo demo2]({{ '/assets/images/team10/demo3.gif'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 4. Demo 3*

Here we also procide a video of a car encountering an obstacle while driving. We can clearly see that when the car encounters an obstacle, it will not recognize it, nor will it consciously avoid it, but will directly hit it. This situation greatly affected the final result.

![video of policy dissection]({{ '/assets/images/team10/metadrive_without.mp4'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## Policy Dissection Result
Here we provide an example of Human-AI shared control through policy dissection method. Through human control, we let the car successfully avoid obstacles and reach the end point.

![video of policy dissection]({{ '/assets/images/team10/metadrive_dissection.mp4'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## PPO with Policy Dissection vs PPO
Here we try three road maps:

![map1]({{ '/assets/images/team10/demo1.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 5. road map 1*

![map2]({{ '/assets/images/team10/demo2.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 6. road map 2*

![map3]({{ '/assets/images/team10/demo3.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 7. road map 3*

| Road Map | Algorithm | Episode Reward | Episode Cost | Arriving Destination |
| ---- | --------- | ----------- | ---------- | ------- |
| 1 | PPO with Policy Dissection | 438.050 | 0.0 | True |
| 1 | PPO | 345.505 | 17.0 | True |
| 2 | PPO with Policy Dissection | 341.361 | 0.0 | True |
| 2 | PPO | 297.871 | 8.0 | True | 
| 3 | PPO with Policy Dissection | 439.840 | 0.0 | True |
| 3 | PPO | 431.896 | 1.0 | True |

*Table 1. All the result on both algorithm*

We compare the result of PPO and PPO with policy dissection, we can find that that the overall performance of PPO with policy dissection is much better. First of all, the rewards of PPO with policy dissection are all higher than that of PPO. In the highest case, the reward can be almost 100 higher. The purpose of reinforcement learning is to maximize the total rewards, which means the policy dissection plays a role and improves the performance. Secondly, PPO always has an episode cost, while the episode cost of PPO with policy dissection is always 0. The episode cost is used to detect the safety of driving, and if a crash occurs, the cost will increses by 1. So, this means that PPO has some collisions during the running process, such as colliding with other cars or obstacles, and polict dissection avoids all obstacles very well. At the end, both algorithms can successfully reache the destination. To sum up, the PPO with policy dissection has much better performance than the PPO, which means that polciy dissection can help to get better result in test, and can also improve the safty of the entire driving.

## Installation
### Basic Installation
We completed the basic installation according to the [
policy dissect's](https://github.com/metadriverse/policydissect.git) instructions. It contains some basic packages we need.

### Environment Installation
We also installed supported environments for testing the policy dissection method.
- [MetaDrive](https://github.com/metadriverse/metadrive.git)

    ![metadrive]({{ '/assets/images/team10/metadrive.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## Reference
[1] Q. Li, Z. Peng, H. Wu, L. Feng, and B. Zhou. Human-AI shared control via policy dissection. *arXiv preprint arXiv:2206.00152*, 2022. 

[2] D. Bau, J.-Y. Zhu, H. Strobelt, A. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual units in a deep neural network. *Proceedings of the National Academy of Sciences*, 117(48):30071–30078, 2020.

[3] S. Guo, R. Zhang, B. Liu, Y. Zhu, D. Ballard, M. Hayhoe, and P. Stone. Machine versus human attention in deep reinforcement learning tasks. *Advances in Neural Information Processing Systems*, 34, 2021.

[4] B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection. *IEEE transactions on pattern analysis and machine intelligence*, 41(9):2131–2145, 2018.

[5] R. Zhang, F. Torabi, G. Warnell, and P. Stone. Recent advances in leveraging human guidance for sequential decision-making tasks. *Autonomous Agents and Multi-Agent Systems*, 35(2):1–39, 2021.

[6] Q. Li, Z. Peng, Z. Xue, Q. Zhang, and B. Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. *arXiv preprint arXiv:2109.12674*, 2021.